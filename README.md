# Mamba 
![](https://assets.anakin.ai/blog/2023/12/0.png)
![Mamba architecture](https://miro.medium.com/v2/resize:fit:1400/1*E349TJjlyuR3IA0Qn445zw.png)

## Overview
This is a PyTorch project that implements a minimalistic U-Mamba architecture [described here](https://arxiv.org/pdf/2312.00752.pdf). 

The model is a replacement for convolutional-based image segmentation models based on state spaces.

For more details architecture, refer to the original paper: [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752).

## Setup

To get started with Transformer Plain, follow these steps:

1. Clone the repository:

    ```shell
    git clone https://github.com/paulilioaica/U-Mamba-Pytorch
    cd Mamba-Pytorch/src/
    ```

2. Install the required dependencies:

    ```shell
    pip install -r requirements.txt
    ```

## Usage

1. Dataset:
   
## Example run


## Results after 10 epochs

## License

This project is licensed under the MIT License. 
